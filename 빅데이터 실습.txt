b1
b2
https://sites.google.com/site/medialoghadoop/01-hadub-gicho/01-hadub-salpyeobogi

# 호스트명 변경 - NameNode가 실행될 host명을 master로 변경
[hadoop@master ~]$ su -
Password : hadoop
[root@master ~] vi /etc/sysconfig/network
HOSTNAME=master
[root@master ~] init 6
재부팅


#JDK 설치
[hadoop@master ~]$ su -
Password : hadoop
[root@master local]# cd /usr/local
[root@master local]# pwd
/usr/local
[root@master local]# tar -xvf  /home/hadoop/Downloads/jdk-7u79-linux-x64.gz
[root@master local]# ls -l
[root@master local]# chown -R hadoop:hadoop /usr/local/jdk1.7.0_79/
[root@master local]# ls -l




#Hadoop설치
[root@master local]# tar -xvf /home/hadoop/Downloads/hadoop-2.7.1.tar.gz
[root@master local]# ls -l
[root@master local]# chown -R hadoop:hadoop /usr/local/hadoop-2.7.1/
[root@master local]# ls -l



#hadoop 계정의 .bash_profile 환경설정
[root@master local]# su - hadoop
[hadoop@master Downloads]$ cd
[hadoop@master ~]$ vi .bash_profile
export PATH=$PATH:$HOME/bin
export JAVA_HOME=/usr/local/jdk1.7.0_79
export HADOOP_HOME=/usr/local/hadoop-2.7.1
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin


--------------------------
위까지 master 설정방법
--------------------------
vmware를 종료 후 slave용으로 세팅된 master vmware를 같은거 한개 더 복사한다.
--------------------------
새로운 vmware를 실행 후 아래 호스트명을 slave1으로 바꾼다.

# 호스트명 변경 - NameNode가 실행될 host명을 slave1로 변경
[hadoop@master ~]$ su -
Password : hadoop
[root@master ~] vi /etc/sysconfig/network
HOSTNAME=slave1
[root@master ~] init 6
재부팅


ip 확인
마스터 : 192.168.118.130
슬레이브1 : 192.168.118.131 

마스터 슬레이브 두개의 vmware에 아래 세팅을한다



/etc/host 파일 설정
[hadoop@master ~]$ su - 
hadoop
[root@master ~]$ vi /etc/hosts
리눅스 시스템에서  ifconfig로 IP를  확인후 변경합니다.


master : 192.168.149.128
slave1 : 192.168.149.129
slave2 : 192.168.149.130
slave3 : 192.168.149.131
slave4 : 192.168.149.128

192.168.149.128 master
192.168.149.129 slave1
192.168.149.130 slave2
192.168.149.131 slave3
192.168.149.128 slave4


-------
SSH 설정 : 개인키 공개키 생성
마스터 작업
-------

 su - hadoop
Password: 
[hadoop@master ~]$ pwd
/home/hadoop
[hadoop@master ~]$ ls .ssh
[hadoop@master ~]$ pwd
/home/hadoop
[hadoop@master ~]$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):  #그냥엔터
Enter passphrase (empty for no passphrase):  #그냥엔터
Enter same passphrase again:  #그냥엔터
Your identification has been saved in /home/hadoop/.ssh/id_rsa.
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.ls
The key fingerprint is:
6f:7a:f2:e5:f0:c5:f8:43:5a:94:3e:3c:ea:6a:d4:66 hadoop@master
The key's randomart image is:
+--[ RSA 2048]----+
|                 |
|                 |
|               . |
|              o  |
|        S  . +   |
|         .. EoB  |
|         .+oo=oo |
|        .oo=oo.  |
|        .=oo+ .. |
+-----------------+
[hadoop@master ~]$ 
[hadoop@master ~]$ 
[hadoop@master ~]$ ls .ssh
id_rsa  id_rsa.pub   #개인키, 공개키 생성된것을 확인함.
[hadoop@master ~]$ 



#공개키를 인증키 파일로 복사
[hadoop@master ~]$ cp ~/.ssh/id_rsa.pub  ~/.ssh/authorized_keys

[hadoop@master ~]$ chmod 755 ~/.ssh
[hadoop@master ~]$ chmod 644 ~/.ssh/authorized_keys
[hadoop@master ~]$ ls .ssh
authorized_keys  id_rsa  id_rsa.pub
[hadoop@master ~]$ ls -l .ssh
total 12
-rw-r--r--. 1 hadoop hadoop  395 May  7 22:29 authorized_keys
-rw-------. 1 hadoop hadoop 1675 May  7 22:26 id_rsa
-rw-r--r--. 1 hadoop hadoop  395 May  7 22:26 id_rsa.pub
[hadoop@master ~]$ 


-------
SSH 설정 : 개인키 공개키 생성
슬레이브1 작업
-------

#slave1 호스트에서 .ssh 디렉토리 확인후 접근 권한 변경y
[hadoop@slave1 ~]$ ls .ssh
[hadoop@slave1 ~]$ chmod 755 ~/.ssh
[hadoop@slave1 ~]$ 


--------
다시 마스터로 가서
-----------

#master 호스트에서 인증키를 클러스터내에 다른 호스트에 분배(slave들 수만큼 작업해주길:slave ip넣을것)
[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@192.168.124.129:./.ssh/
The authenticity of host '192.168.124.129 (192.168.124.129)' can't be established.
RSA key fingerprint is f8:07:bc:1a:99:e1:a6:2b:ef:04:09:6d:35:b7:10:ea.
Are you sure you want to continue connecting (yes/no)? yes                  #yes입력
Warning: Permanently added '192.168.124.129' (RSA) to the list of known hosts.
hadoop@192.168.124.129's password: 
authorized_keys                               100%  395     0.4KB/s   00:00 



#master 서버에서 테스트
$ ssh hadoop@master date
$ ssh hadoop@secondary date
$ ssh hadoop@slave1 date
$ ssh hadoop@slave2 date

[hadoop@master ~]$ cd /usr/local/hadoop-2.7.1
[hadoop@master hadoop-2.7.1]$ cd etc/hadoop
[hadoop@master hadoop]$ ls
capacity-scheduler.xml      kms-env.sh
configuration.xsl           kms-log4j.properties
container-executor.cfg      kms-site.xml
core-site.xml               log4j.properties
hadoop-env.cmd              mapred-env.cmd
hadoop-env.sh               mapred-env.sh
hadoop-metrics2.properties  mapred-queues.xml.template
hadoop-metrics.properties   mapred-site.xml.template
hadoop-policy.xml           slaves
hdfs-site.xml               ssl-client.xml.example
httpfs-env.sh               ssl-server.xml.example
httpfs-log4j.properties     yarn-env.cmd
httpfs-signature.secret     yarn-env.sh
httpfs-site.xml             yarn-site.xml
kms-acls.xml
[hadoop@master hadoop]$ 




hadoop-env.sh 설정 - 하둡을 실행하는 쉘 스크립트 파일들에서 필요로 하는 환경변수들을 설정

[hadoop@master ~]$ cd /usr/local/hadoop-2.7.1/etc/hadoop
 
[hadoop@master ~]$ vi hadoop-env.sh
export JAVA_HOME=/usr/local/jdk1.7.0_79
자바 설치된 경로 확인 후 설정하세요

하둡을 설치하고 테스트 할 때 아래와 같이 경고메시지가 계속 뜰 수 있습니다.
“Warning: $HADOOP_HOME is deprecated”
이럴 때 아래의 내용을 hadoop-env.sh 파일에 추가해 주세요.
export HADOOP_HOME_WARN_SUPPRESS=1

 

#core-site.xml 파일 수정
HDFS와 맵리듀스에 공통적으로 사용되는 IO와 같은 하둡 코어를 위한 환경을 설정하는 파일
로그파일, 네크워크 튜닝, I/O 튜닝, 파일 시스템 튜닝, 압축 등 하부 시스템 설정 파일

[hadoop@master hadoop]$ vi core-site.xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!-- Put site-specific property overrides in this file. -->
<configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://master:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/usr/local/hadoop-2.7.1/tmp</value>
</property>
</configuration>


# hdfs-site.xml 설정
 
dfs.replication 값이 1일때 마스터와 가까운 slave2에 저장하고 임계치가 지나야 slave1과 병행하여 저장한다.
임계치 설정은 dfsadmin으로 한다. 115페이지

2면 양쪽저장  3이 디폴트


[hadoop@master ~]$ cd /usr/local/hadoop-2.7.1/etc/hadoop/
[hadoop@master hadoop]$ vi hdfs-site.xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!-- Put site-specific property overrides in this file. -->
<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.permissions.enabled</name>
<value>false</value>
</property>
<property>
<name>dfs.secondary.http.address</name>
<value>secondary:50090</value>
</property>
</configuration>






#mapred-site.xml 파일 수정
mapred-site.xml 파일은 맵리듀스에서 사용할 환경정보를 설정합니다. hadoop-core-x.x.x.jar
파일에 포함되어 있는 mapred-default.xml을 오버라이드 한 파일입니다. mapred-site.xml에 설
정 값이 없을 경우 mapred-default.xml에 있는 기본 값을 사용합니다.

[hadoop@master hadoop]$ cp mapred-site.xml.template mapred-site.xml
[hadoop@master hadoop]$ vi mapred-site.xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>



#yarn-site.xml 파일 수정
맵리듀스 프레임워크에서 사용하는 셔플 서비스를 지정한다.

[hadoop@master hadoop]$ vi yarn-site.xml
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
</configuration>



#데이터노드 설정
[hadoop@master hadoop]$ cd /usr/local/hadoop-2.7.1/etc/hadoop/
[hadoop@master ~]$ vi slaves
slave1
slave2 


#hadoop-env.sh파일에 설정된 hadoop tmp 디렉토리 구성
[hadoop@master hadoop]$ mkdir -p /usr/local/hadoop-2.7.1/tmp
[hadoop@master hadoop]$ mkdir -p /usr/local/hadoop-2.7.1/tmp/dfs
[hadoop@master hadoop]$ mkdir -p /usr/local/hadoop-2.7.1/tmp/dfs/name
[hadoop@master hadoop]$ mkdir -p /usr/local/hadoop-2.7.1/tmp/dfs/data
[hadoop@master hadoop]$ mkdir -p /usr/local/hadoop-2.7.1/tmp/mapred
[hadoop@master hadoop]$ mkdir -p /usr/local/hadoop-2.7.1/tmp/mapred/system
[hadoop@master hadoop]$ mkdir -p /usr/local/hadoop-2.7.1/tmp/mapred/local
[hadoop@master hadoop]$ chmod 755 /usr/local/hadoop-2.7.1/tmp/dfs





기본설정 끝







#마스터 노드에서 하둡 설정 파일들을 하둡 클러스터내에 다른 노드들에 복제 수행

#다른 pc 로 구성된 것들만 동기화 해준다. 우리가 테스트한 환경은 slave1만 다른 pc로 구성되어있음
#slave2는 master 같은 pc이기에 안해줘도 된다. 다르pc면 똑같이 해준다.

[hadoop@master ~] cd /usr/local/hadoop-2.7.1
rsync -av . hadoop@slave1:/usr/local/hadoop-2.7.1

[hadoop@master ~] cd /usr/local/hadoop-2.7.1/etc/hadoop
rsync -av . hadoop@slave1:/usr/local/hadoop-2.7.1/etc/hadoop









==============
마스터, 슬레이브 전부 설정
su - 
루트 계정으로 
==============


◈ 방화벽 설정은 모든 노드에서 수행되어야 한다. 즉, datanode 뿐만 아니라
namenode 에도 방화벽 규칙이 추가돼 있어야 한다.
◈ # vi /etc/sysconfig/iptables
-A INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 8088 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 50070 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 50090 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 9000 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 5432 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 3306 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 8032 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 12000 -j ACCEPT
-A INPUT -s 192.168.118.0/24 -d 192.168.118.0/24 -j ACCEPT
-A OUTPUT -s 192.168.118.0/24 -d 192.168.118.0/24 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT


#방화벽 재 실행
service iptables restart


#하둡 설정 확인



#네임노드 초기화
#네임노드는 최초 한번만 실행하면 됨
[hadoop@master hadoop]$ cd /usr/local/hadoop-2.7.1/bin
[hadoop@master hadoop]$ hdfs namenode -format






#hdfs 실행
[hadoop@master hadoop]$  cd ..
[hadoop@master hadoop]$  sbin/start-all.sh
[hadoop@master hadoop]$  sbin/mr-jobhistory-daemon.sh start historyserver

#프로세스 데몬 확인
[hadoop@master hadoop]$  jps

브라우저 열고
http://master:50070/dfshealth.html



#hdfs 종료
[hadoop@master hadoop]$  sbin/stop-all.sh


-------------------------------------------------

[hadoop@master hadoop]$  sbin/hadoop-daemon.sh start namenode
[hadoop@master hadoop]$  sbin/hadoop-daemons.sh start datanode
[hadoop@master hadoop]$  sbin/yarn-daemon.sh start resourcemanager
[hadoop@master hadoop]$  sbin/yarn-daemons.sh start nodemanager
[hadoop@master hadoop]$  sbin/mr-jobhistory-daemon.sh start historyserver
 브라우저에서 http://master:50070/dfshealth.jsp 실행 후 파일 시스템 상태 보여야 함
 1.x의 JobTracker는 http://master:8088/cluster 에서 확인할 수 있음
--------------------------------------------------


#하둡 루트의 파일 리스트 보기
[hadoop@master ~]$ hadoop fs -ls /
[hadoop@master ~]$ hadoop fs -ls -R /  #전체보기

#파일복사
[hadoop@master ~]$ hadoop fs -put ./Downloads/2008.csv /usr/data/

128메가로 해시값으로 복사됨


하둡 디렉토리

usr/local/하둡설치폴더/tmp/dfs/data/current/해시폴더/current/finalized/subdir0/subdir0

#매뉴얼
[hadoop@master ~]$ hadoop dfsadmin help 

#검사
[hadoop@master ~]$ hadoop fsck /




#hdfs-site.xml   dfs.replication 값이 1일때 임계치 설정

[hadoop@master hadoop-2.7.1]$ ls
bin  include  libexec      logs        README.txt  share
etc  lib      LICENSE.txt  NOTICE.txt  sbin        tmp
[hadoop@master hadoop-2.7.1]$ pwd
/usr/local/hadoop-2.7.1
[hadoop@master hadoop-2.7.1]$ sbin/start-balancer.sh -threshold 1
starting balancer, logging to /usr/local/hadoop-2.7.1/logs/hadoop-hadoop-balancer-master.out
[hadoop@master hadoop-2.7.1]$ 



[hadoop@master local]$ bin/start-balancer.sh -threshold 1   #숫자는 % 를 뜻함










===================================
mapreduce 실습
==================================
[hadoop@master ~]$ su - 

[root@master ~]# cd /home/hadoop
[root@master hadoop]# tar -xvf /home/hadoop/Downloads/eclipse-jee-mars-1-linux-gtk-x86_64.tar.gz
[root@master hadoop]# ls -al
[root@master hadoop]# chown -R hadoop:hadoop ./eclipse
[root@master hadoop]# ls -al

=============================================
WordCountMapper.java
=============================================
package hadoop.sample.wordcount;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordCountMapper extends
		Mapper<LongWritable, Text, Text, IntWritable> {

	private final static IntWritable one = new IntWritable(1);
	private Text word = new Text();

	public void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {
		StringTokenizer itr = new StringTokenizer(value.toString());
		while (itr.hasMoreTokens()) {
			word.set(itr.nextToken());
			context.write(word, one);
		}
	}
}

=============================================
WordCountReducer.java
=============================================
package hadoop.sample.wordcount;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordCountReducer extends
		Reducer<Text, IntWritable, Text, IntWritable> {
	private IntWritable result = new IntWritable();

	public void reduce(Text key, Iterable<IntWritable> values, Context context)
			throws IOException, InterruptedException {
		int sum = 0;
		for (IntWritable val : values) {
			sum += val.get();
		}
		result.set(sum);
		context.write(key, result);
	}
}

===============================================
WordCount.java
===============================================
package hadoop.sample.wordcount;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class WordCount {
	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		if (args.length != 2) {
			System.err.println("Usage: WordCount <input> <output>");
			System.exit(2);
		}
		Job job = new Job(conf, "WordCount");

		job.setJarByClass(WordCount.class);
		job.setMapperClass(WordCountMapper.class);
		job.setReducerClass(WordCountReducer.class);

		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);

		// 파일 시스템 제어 객체 생성
		FileSystem hdfs = FileSystem.get(conf);
		// 경로 체크
		Path path = new Path(args[1]);
		if (hdfs.exists(path)) {
			hdfs.delete(path, true);
		}
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		job.waitForCompletion(true);
	}
}


#실행
[hadoop@master ~]$ hadoop fs put ./input.txt /usr/data/
[hadoop@master ~]$ hadoop fs -ls -R /
[hadoop@master ~]$ hadoop jar wordcount.jar  /usr/data/input.txt  /output/
[hadoop@master ~]$ hadoop fs -cat  /output/part-r-00000

#실행
[hadoop@master ~]$ hadoop fs put ./Downloads/2007.csv /usr/data/
[hadoop@master ~]$ hadoop fs -ls -R /
[hadoop@master ~]$ hadoop jar delaycount.jar  /usr/data/2007.csv  /output/
[hadoop@master ~]$ hadoop fs -cat  /output/part-r-00000  #?







 ======================================
hive실습
=======================================

#Hive의 metastore로 사용할 mysql 설치
[hadoop@master ~]$ su -
password : hadoop   
 
[root@master ~]# rpm -ivh /home/hadoop/Downloads/mysql-community-release-el6-5.noarch.rpm
[root@master ~]# ls -la /etc/yum.repos.d/
# MySQL 5.6 설치
[root@master ~]# yum install mysql-server


[root@master ~]# ls /usr/bin/mysql
[root@master ~]# ls /usr/sbin/mysqld


#mysql 실행
[root@master ~]# service mysqld start
[root@master ~]# mysql --version
[root@master ~]# netstat -anp | grep mysql

#hive 설치

[root@master ~]# cd /usr/local/
[root@master local]# tar -xvf /home/hadoop/Downloads/apache-hive-1.1.1-bin.tar.gz 
[root@master local]# ls -l
 
[root@master local]# chown -R hadoop:hadoop apache-hive-1.1.1-bin/
[root@master local]# ln -s apache-hive-1.1.1-bin/  hive
[root@master local]# ls -l 
[root@master local]# chown -R hadoop:hadoop hive
[root@master local]# ls -l


#hadoop계정의 환경변수 설정 파일에 추가
[root@master local]# su - hadoop
[hadoop@master ~]$ vi .bash_profile

export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:

# Hive에서 사용할 HDFS 에 디렉토리 구성하기 
[hadoop@master ~]$ cd /usr/local/hadoop-2.7.1
[hadoop@master ~]$ sbin/start-all.sh
[hadoop@master ~]$ jps


[hadoop@master ~]$ hadoop fs -mkdir -p /user/hive/warehouse
[hadoop@master ~]$ hadoop fs -ls -R /user/


#hive 설정 -hive-env.sh 생성
[hadoop@master ~]$ cd /usr/local/hive/conf/
[hadoop@master ~]$ cp hive-env.sh.template  hive-env.sh
[hadoop@master ~]$ vi hive-env.sh
HADOOP_HOME=/usr/local/hadoop-2.7.1
[hadoop@master ~]$  chmod 755 hive-env.sh
[hadoop@master ~]$ vi hive-site.xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

<property>
  <name>hive.metastore.local</name>
  <value>true</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/metastore_db?createDatabaseIfNotExist=true</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
  <description>username to use against metastore database</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
  <description>password to use against metastore database</description>
</property> 
</configuration>





#  mysql에  hive사용자 및 metastore용 DB 생성 
[root@master local]#  mysql -u root -p
Enter password:                         #패스워드 없으므로 그냥 엔터
mysql> show databases;
mysql> CREATE DATABASE metastore_db;
mysql> USE metastore_db;
mysql> show tables;
mysql> SOURCE /usr/local/hive/scripts/metastore/upgrade/mysql/hive-schema-1.1.0.mysql.sql;
mysql> show tables;

# hive에서 루트 사용자로 사용할 User와 암호를 설정한다.
mysql> create user 'hive'@'%' identified by 'hive';
mysql> grant all on *.* to 'hive'@localhost identified by 'hive';
mysql> flush privileges; 

#jdbc 드라이버 클래스 hive 라이브러리에 추가
$HIVE_HOME/lib 아래 mysql-connector-java-5.x.x.jar에 복사

hadoop fs -mkdir -p /usr/data

hadoop fs -ls -R /

#hive 실행
[hadoop@master ~]$ hive
hive> CREATE EXTERNAL TABLE airline (
Year string,
Month string,
DayofMonth string,
DayOfWeek string,
DepTime string,
CRSDepTime string,
ArrTime string,
CRSArrTime string,
UniqueCarrier string,
FlightNum string,
TailNum string,
ActualElapsedTime string,
CRSElapsedTime string,
AirTime string,
ArrDelay string,
DepDelay string,
Origin string,
Dest string,
Distance string,
TaxiIn string,
TaxiOut string,
Cancelled string,
CancellationCode string,
Diverted string,
CarrierDelay string,
WeatherDelay string,
NASDelay string,
SecurityDelay string,
LateAircraftDelay  string
)
ROW FORMAT DELIMITED
 FIELDS TERMINATED BY ',' 
 LINES TERMINATED BY '\n'
LOCATION '/data/';

[hadoop@master ~]$ hadoop fs -rm /usr/data/input.txt

#생성된 테이블 확인
hive> show tables
mysql> select TBL_ID, TBL_NAME, DB_ID from TBLS;

#월별 출발 지연 횟수
hive> SELECT Month, count(DepDelay)
      FROM airline
      GROUP BY Month
      ORDER BY Month;


hive>explain SELECT Year,Month, count(DepDelay)
FROM airline
GROUP BY Year,Month
ORDER BY Year,Month;


hive>explain SELECT Year,Month, count(DepDelay)
FROM airline
GROUP BY Year,Month
SORT BY Year,Month;



# [hadoop@master ~]$ vi /home/hadoop/dept.txt 
10,'ACCOUNTING','NEW YORK'
20,'RESEARCH','DALLAS'
30,'SALES','CHICAGO'
40,'OPERATIONS','BOSTON'

hive>CREATE TABLE IF NOT EXISTS dept (
deptno INT, dname STRING, loc STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

hive>describe dept

#생성한 테이블 이름의 디렉토리 확인
[hadoop@master ~]$ hadoop fs -ls -R /user/hive/warehouse/


hive>load data local inpath '/home/hadoop/dept.txt' 
overwrite into table dept;

hive>select  * from dept;

#dept테이블의 데이터가 dept디렉토리 아래 저장되었음을 확인
[hadoop@master ~]$ hadoop fs -ls -R /user/hive/warehouse/

mysql> use metastore_db
mysql> describe TBLS
mysql> select TBL_ID, DB_ID, TBL_NAME, TBL_TYPE
    -> from TBLS;

hive> INSERT  OVERWRITE  TABLE  dept 
    > select * from dept;
hive>select  * from dept;
#dept테이블의 데이터가 dept디렉토리 아래 저장되었음을 확인
[hadoop@master ~]$ hadoop fs -ls -R /user/hive/warehouse/
[hadoop@master ~]$ hadoop fs -cat /user/hive/warehouse/dept/000000_0






#####################################
R설치
#####################################

[root@master ~]# yum install epel-release
[root@master ~]# yum install npm
[root@master ~]# yum install R 
[root@master ~]# ls -al /usr/lib64/R
[root@master ~]# chown -R hadoop:hadoop /usr/lib64/R
[root@master ~]# ls -al /usr/lib64/R
[root@master ~]# su - hadoop
#R 실행
[hadoop@master ~]# R
> print("hello")
> seq(1:100)
> ?print
> ??print

# vi /home/hadoo/data_load.R 
print("hello")
x = seq(1:100)
print(x)


> source("/home/hadoo/data_load.R")
> install.packages("randomForest")
> library(randomForest)
> m <- randomForest(Species ~., data=iris)
> m

>cat(1, 'a', 2, 'b')
>1+2 ; 3*2 ; 4/2
> 5/3   #실수 결과
> 5%/%3   # 정수 결과
> 5%%3  
> 5**3
> 5^3

> class('1')
> class(1)

>sum(1, NA, 2)  # NA를 더하므로 결과가 NA로 출력됩니다.
>sum(1, NULL, 2)   #NULL값을 제외하고 나머지 값만 더해서 결과 출력
>sum(1, 2, NA, na.rm=T)  # NA값을 제거하고 올바른 계산을 수행

Sys.Date()   # 날짜만 보여주는 함수
Sys.time()    # 날짜와 시간을 보여주는 함수
date()         # 미국식 날짜와 시간을 출력하는 함수
as.Date('2017-12-01')    # 문자형태의 날짜를 날짜타입으로 변환해주는 함수
as.Date('2017-12-01'  ,  format='%d-%m-%Y')
as.Date(10, origin='2017-12-01')   #주어진 날짜 기준으로 10일후의 날짜
as.Date(-10, origin='2017-12-01')  #주어진 날짜 기준으로 10일 이전 날짜



vec 1 <- c(1, 2, 3, 4, 5)
vec1[-3]
vec1 [3]
vec 2 <- c(10, 9, 8, 7, 6)
vec3 <-append(vec1, vec2, after=3)
result <- vec3[2:7]
result
vec1+vec2
names(vec1) <- c('key1', 'key2', 'key3', 'key4', 'key5')
var1 <- seq(2, -2)
var2 <- seq(1, 10, 2)
var3 <- rep(1:3, 2)
length(var2)
nrow(var2)
3 %in% var2 


dlink-4584
uowqn28127

#hadoop의 .bash_profile에 추가
export HADOOP_CMD=/usr/local/hadoop-2.7.1/bin/hadoop
export HADOOP_STREAMING=/usr/local/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar

로그 오프 후에 다시 로그 온 한후 
R 실행시킴
>install.packages(c("rJava", "Rcpp", "RJSONIO", "bitops", "digest", "functional", "stringr", "plyr", "reshape2", "caTools"))
>install.packages("/home/hadoop/Downloads/rhdfs_1.0.8.tar.gz", repos=NULL, type="source")
>install.packages("/home/hadoop/Downloads/rmr2_3.3.1.tar.gz", repos=NULL, type="source")

# 1부터 1,000까지의 숫자를 생성/ 각 숫자 모두를 제곱하는 연산을 수행 
[hadoop@master ~] hadoop fs -mkdir /tmp/ex1

>library(rhdfs) # Rhadoop package for hdfs
>hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
>library(rmr2)  # RHadoop package for MapReduce
> dfs.rmr("/tmp/ex1")
> small.ints <- to.dfs(1:1000, "/tmp/ex1")

> result <- mapreduce(input = small.ints, 
	map = function(k,v) cbind(v,v^2)
)
> out <- from.dfs(result)
> out


#균일분포에서 1000개씩 난수를 발생 평균계산/ 1000번 반복 후 평균의 히스토그램
[hadoop@master ~] hadoop fs -mkdir /tmp/ex2

> library(rhdfs) # Rhadoop package for hdfs
> hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
> library(rmr2)  # RHadoop package for MapReduce
 
> infile <- "/tmp/ex2"
> if(dfs.exists(infile)) dfs.rmr(infile)
> small.ints = to.dfs(1:1000, output=infile)
## Defining the MapReduce job 
> test <- mapreduce(
  input = small.ints,
  map = function(k, v){
    lapply(seq_along(v), function(r){
      x <- runif(1000)
      keyval(r, mean(x))
    }
    )
  }
)
> output <- from.dfs(test)
# 리스트의 2번째 값만 추출
> Means <- do.call("c", lapply(output$val, "[[", 2)) 
> hist(Means)


# 문서자료에서 단어빈도계산
[hadoop@master ~] hadoop fs -put /tmp/README.txt
[hadoop@master ~] hadoop fs -mkdir /tmp/ex4

library(rhdfs) # Rhadoop package for hdfs
hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
library(rmr2)  # RHadoop package for MapReduce
 
inputfile <- "/tmp/README.txt"
if(!hdfs.exists(inputfile)) stop("File is not found")
outputfile <- "/tmp/ex4"
if(hdfs.exists(outputfile)) hdfs.rm(outputfile)
 
map <- function(key, val){
	words.vec <- unlist(strsplit(val, split = " "))
	#lapply(words.vec, function(word) 
    keyval(words.vec, 1)
}
 
reduce <- function(word, counts ) {
	keyval(word, sum(counts))
}

result <- mapreduce(input = inputfile,
	output = outputfile, 
	input.format = "text", 
	map = map, 
	reduce = reduce, 
	combine = T
)
 
## wordcount output
freq.dfs <- from.dfs(result)
freq <- freq.dfs$val
word <- freq.dfs$key
oidx <- order(freq, decreasing=T)[1:10]
 
# Words frequency plot
barplot(freq[oidx], names.arg=word[oidx] )